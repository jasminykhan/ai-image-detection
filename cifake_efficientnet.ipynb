{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-12T13:20:09.878645Z",
     "iopub.status.busy": "2023-05-12T13:20:09.878071Z",
     "iopub.status.idle": "2023-05-12T13:20:10.179387Z",
     "shell.execute_reply": "2023-05-12T13:20:10.178413Z",
     "shell.execute_reply.started": "2023-05-12T13:20:09.878597Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import time\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.layers import Dense, Activation,Dropout,Conv2D, MaxPooling2D,BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam, Adamax\n",
    "from tensorflow.keras.metrics import categorical_crossentropy\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import backend as K\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style('darkgrid')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### below is a handy function that enables the printing of test in user specified foreground and background colors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-12T11:37:51.000554Z",
     "iopub.status.busy": "2023-05-12T11:37:50.99981Z",
     "iopub.status.idle": "2023-05-12T11:37:51.008955Z",
     "shell.execute_reply": "2023-05-12T11:37:51.007865Z",
     "shell.execute_reply.started": "2023-05-12T11:37:51.000518Z"
    }
   },
   "outputs": [],
   "source": [
    "def print_in_color(txt_msg,fore_tupple= (0,255,255) ,back_tupple=(0,0,0)):\n",
    "    #prints the text_msg in the foreground color specified by fore_tupple with the background specified by back_tupple \n",
    "    #text_msg is the text, fore_tupple is foregroud color tupple (r,g,b), back_tupple is background tupple (r,g,b)\n",
    "    rf,gf,bf=fore_tupple\n",
    "    rb,gb,bb=back_tupple\n",
    "    msg='{0}' + txt_msg\n",
    "    mat='\\33[38;2;' + str(rf) +';' + str(gf) + ';' + str(bf) + ';48;2;' + str(rb) + ';' +str(gb) + ';' + str(bb) +'m' \n",
    "    print(msg .format(mat))\n",
    "    print('\\33[0m', end='') # returns default print color to back to black"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset has a train directory with 50,00 real and 50,000 fake images. Lets look at the size of a typical image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-12T11:37:54.341293Z",
     "iopub.status.busy": "2023-05-12T11:37:54.340866Z",
     "iopub.status.idle": "2023-05-12T11:37:54.350696Z",
     "shell.execute_reply": "2023-05-12T11:37:54.349109Z",
     "shell.execute_reply.started": "2023-05-12T11:37:54.34126Z"
    }
   },
   "outputs": [],
   "source": [
    "fpath=r'/ai-image-detection/data/train/REAL/0000 (10).jpg'\n",
    "img=cv2.imread(fpath)\n",
    "print('Image shape is ', img.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Images are small but there are 100,00 training images. I will set a limiter so that only 20,000 images are used for each of the two classes, the information in a DataFrames.  10% of the train images  to be used for a validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-12T11:38:31.834263Z",
     "iopub.status.busy": "2023-05-12T11:38:31.833867Z",
     "iopub.status.idle": "2023-05-12T11:38:32.26728Z",
     "shell.execute_reply": "2023-05-12T11:38:32.266272Z",
     "shell.execute_reply.started": "2023-05-12T11:38:31.834229Z"
    }
   },
   "outputs": [],
   "source": [
    "limiter = 20000\n",
    "test_dir=r'/ai-image-detection/data/test'\n",
    "train_dir=r'/ai-image-detection/data/train'\n",
    "classes=sorted(os.listdir(train_dir))\n",
    "print('there are ', len(classes), ' classes ', classes[0], ' and ', classes[1])\n",
    "dir_list=[train_dir, test_dir]\n",
    "names=['train', 'test']\n",
    "zip_list=zip(names, dir_list)\n",
    "for name, dir in zip_list:\n",
    "    filepaths=[]\n",
    "    labels=[]\n",
    "    class_list=sorted(os.listdir(dir))\n",
    "    for klass in class_list:\n",
    "        classpath=os.path.join(dir, klass)\n",
    "        flist=sorted(os.listdir(classpath))\n",
    "        if name == 'train':\n",
    "            flist=np.random.choice(flist, limiter, replace=False) # randomly select limiter number of files from train_dir for each class\n",
    "        desc=f'{name}-{klass}'\n",
    "        for f in tqdm(flist, ncols=100, colour='blue', unit='files', desc=desc):\n",
    "            fpath=os.path.join(classpath,f)\n",
    "            filepaths.append(fpath)\n",
    "            labels.append(klass)\n",
    "    Fseries=pd.Series(filepaths, name='filepaths')\n",
    "    Lseries=pd.Series(labels, name='labels')\n",
    "    if name == 'train':\n",
    "        train_df=pd.concat([Fseries, Lseries], axis=1)\n",
    "        train_df, valid_df=train_test_split(train_df, test_size=.1, shuffle=True, random_state=123, stratify=train_df['labels'])\n",
    "    else:  \n",
    "        test_df=pd.concat([Fseries, Lseries], axis=1)\n",
    "print('train_df length: ', len(train_df), '  test_df length: ', len(test_df), '  valid_df length: ', len(valid_df)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Since that train data is balance and has a large number of images we do not need to do augmentation. Create the train. test and valid generators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-12T11:38:52.04826Z",
     "iopub.status.busy": "2023-05-12T11:38:52.047879Z",
     "iopub.status.idle": "2023-05-12T11:40:35.497396Z",
     "shell.execute_reply": "2023-05-12T11:40:35.496297Z",
     "shell.execute_reply.started": "2023-05-12T11:38:52.04823Z"
    }
   },
   "outputs": [],
   "source": [
    "gen=ImageDataGenerator()\n",
    "# I will be using an EfficientNet model which requires a minimum image size of 32 X 32\n",
    "img_size=(32,32)\n",
    "bs=200 # set the batch size\n",
    "train_gen=gen.flow_from_dataframe(train_df, x_col='filepaths', y_col='labels', target_size=img_size,\n",
    "                                 class_mode= 'categorical', color_mode='rgb', shuffle=True, batch_size=bs)\n",
    "valid_gen=gen.flow_from_dataframe(valid_df, x_col='filepaths', y_col='labels', target_size=img_size,\n",
    "                                 class_mode= 'categorical', color_mode='rgb', shuffle=False, batch_size=bs)\n",
    "test_gen=gen.flow_from_dataframe(test_df, x_col='filepaths', y_col='labels', target_size=img_size,\n",
    "                                 class_mode= 'categorical', color_mode='rgb', shuffle=False, batch_size=bs)\n",
    "labels=test_gen.labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I will define a custom metric called F1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-12T11:40:47.580026Z",
     "iopub.status.busy": "2023-05-12T11:40:47.57966Z",
     "iopub.status.idle": "2023-05-12T11:40:47.5879Z",
     "shell.execute_reply": "2023-05-12T11:40:47.586774Z",
     "shell.execute_reply.started": "2023-05-12T11:40:47.579998Z"
    }
   },
   "outputs": [],
   "source": [
    "def F1_score(y_true, y_pred): #taken from old keras source code\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    f1_val = 2*(precision*recall)/(precision+recall+K.epsilon())\n",
    "    return f1_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### now lets use transfer learning with an EfficientNetB0 model. You are ofter told to initially make the base model not trainable, run some epochs then make the base model trrainable and run more epochs to fine tune the model. I disagree with that approach. I have found it is better to make the base model trainable from the outset. My testing indicates I get to a higher validation accuracy in less epochs than doing the fine tuning approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-12T11:41:11.833621Z",
     "iopub.status.busy": "2023-05-12T11:41:11.833254Z",
     "iopub.status.idle": "2023-05-12T11:41:14.124321Z",
     "shell.execute_reply": "2023-05-12T11:41:14.12327Z",
     "shell.execute_reply.started": "2023-05-12T11:41:11.833593Z"
    }
   },
   "outputs": [],
   "source": [
    "img_shape=(img_size[0], img_size[1], 3)\n",
    "base_model=tf.keras.applications.EfficientNetV2B0(include_top=False, weights=\"imagenet\",input_shape=img_shape, pooling='max')\n",
    "base_model.trainable=True\n",
    "x=base_model.output\n",
    "x=BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001 )(x)\n",
    "x = Dense(256, kernel_regularizer = regularizers.l2(l = 0.016),activity_regularizer=regularizers.l1(0.006),\n",
    "                bias_regularizer=regularizers.l1(0.006) ,activation='relu')(x)\n",
    "x=Dropout(rate=.4, seed=123)(x)       \n",
    "output=Dense(2, activation='softmax')(x)\n",
    "model=Model(inputs=base_model.input, outputs=output)\n",
    "model.compile(Adamax(learning_rate=.001), loss='categorical_crossentropy', metrics=['accuracy', F1_score]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Before training the model I want to utilize a custom callback I developed.\n",
    "The LR_ASK callback is a convenient callback that allows you to continue training for ask_epoch more epochs or to halt training.  \n",
    "If you elect to continue training for more epochs you are given the option to retain the current learning rate (LR) or to  \n",
    "enter a new value for the learning rate. The form of use is:  \n",
    "ask=LR_ASK(model,epochs, ask_epoch) where:  \n",
    "* model is a string which is the name of your compiled model\n",
    "* epochs is an integer which is the number of epochs to run specified in model.fit\n",
    "* ask_epoch is an integer. If ask_epoch is set to a value say 5 then the model will train for 5 epochs.  \n",
    "  then the user is ask to enter H to halt training, or enter an inter value. For example if you enter 4  \n",
    "  training will continue for 4 more epochs to epoch 9 then you will be queried again. Once you enter an  \n",
    "  integer value you are prompted to press ENTER to continue training using the current learning rate  \n",
    "  or to enter a new value for the learning rate.\n",
    " * dwell is a boolean. If set to true the function compares the validation loss for the current tp the lowest   \n",
    "   validation loss thus far achieved. If the validation loss for the current epoch is larger then learning rate  \n",
    "   is automatically adjust by the formulanew_lr=lr * factor where factor is a float between 0 and 1. The motivation  \n",
    "   here is that if the validation loss increased we have moved to a point in Nspace on the cost functiob surface that  \n",
    "   if less favorable(higher cost) than for the epoch with the lowest cost. So the model is loaded with the weights\n",
    "   from the epoch with the lowest loss and the learning rate is reduced\n",
    "  \n",
    " At the end of training the model weights are set to the weights for the epoch that achieved the lowest validation loss.\n",
    " The callback also prints the training data in a spreadsheet type of format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-12T11:41:18.913877Z",
     "iopub.status.busy": "2023-05-12T11:41:18.913327Z",
     "iopub.status.idle": "2023-05-12T11:41:18.941725Z",
     "shell.execute_reply": "2023-05-12T11:41:18.940668Z",
     "shell.execute_reply.started": "2023-05-12T11:41:18.913844Z"
    }
   },
   "outputs": [],
   "source": [
    "class LR_ASK(keras.callbacks.Callback):\n",
    "    def __init__ (self, model, epochs,  ask_epoch, batches, dwell=True, factor=.4): # initialization of the callback\n",
    "        super(LR_ASK, self).__init__()\n",
    "        self.model=model               \n",
    "        self.ask_epoch=ask_epoch\n",
    "        self.epochs=epochs\n",
    "        self.ask=True # if True query the user on a specified epoch\n",
    "        self.lowest_vloss=np.inf\n",
    "        self.lowest_aloss=np.inf\n",
    "        self.best_weights=self.model.get_weights() # set best weights to model's initial weights\n",
    "        self.best_epoch=1        \n",
    "        self.dwell= dwell\n",
    "        self.factor=factor\n",
    "        self.header=True\n",
    "        self.batches=batches\n",
    "    \n",
    "    def on_train_begin(self, logs=None): # this runs on the beginning of training \n",
    "        msg1 =f'Training will proceed until epoch {self.ask_epoch} then you will be asked to\\n'\n",
    "        msg2='enter H to halt training or enter an integer for how many more epochs to run then be asked again'\n",
    "        print_in_color(msg1 + msg2)\n",
    "        if self.dwell:\n",
    "            msg='learning rate will be automatically adjusted during training'\n",
    "            print_in_color(msg, (0,255,0))\n",
    "        self.start_time= time.time() # set the time at which training started\n",
    "       \n",
    "    def on_train_end(self, logs=None):   # runs at the end of training  \n",
    "        msg=f'loading model with weights from epoch {self.best_epoch}'\n",
    "        print_in_color(msg, (0,255,255))\n",
    "        self.model.set_weights(self.best_weights) # set the weights of the model to the best weights\n",
    "        tr_duration=time.time() - self.start_time   # determine how long the training cycle lasted         \n",
    "        hours = tr_duration // 3600\n",
    "        minutes = (tr_duration - (hours * 3600)) // 60\n",
    "        seconds = tr_duration - ((hours * 3600) + (minutes * 60))\n",
    "        msg = f'training elapsed time was {str(hours)} hours, {minutes:4.1f} minutes, {seconds:4.2f} seconds)'\n",
    "        print_in_color (msg) # print out training duration time\n",
    "   \n",
    "    def on_epoch_begin(self, epoch, logs= None):\n",
    "        self.ep_start = time.time()\n",
    "    def on_train_batch_end(self, batch, logs= None):\n",
    "        # get batch accuracy and loss\n",
    "        acc = logs.get('accuracy') * 100\n",
    "        loss = logs.get('loss')\n",
    "        # prints over on the same line to show running batch count\n",
    "        msg = '{0:20s}processing batch {1:} of {2:5s}- accuracy=  {3:5.3f} - loss: {4:8.5f}          '.format(' ', str(batch), str(self.batches), acc, loss)\n",
    "        print(msg, '\\r', end= '')\n",
    "        \n",
    "    def on_epoch_end(self, epoch, logs=None):  # method runs on the end of each epoch\n",
    "        if self.header == True:\n",
    "            msg = '{0:^7s}{1:^9s}{2:^9s}{3:^9s}{4:^10s}{5:^13s}{6:^10s}{7:^13s}{8:13s}\\n'\n",
    "            msg1=msg.format('Epoch', 'Train', 'Train', 'Valid', 'Valid','V_Loss %', 'Learning','Next LR' ,'Duration in')\n",
    "            msg='{0:^7s}{1:^9s}{2:^9s}{3:^9s}{4:^10s}{5:^13s}{6:^10s}{7:^13s}{8:13s}'\n",
    "            msg2=msg.format(' ', 'Loss', 'Accuracy', 'Loss', 'Accuracy','Improvement', 'Rate', 'Rate', '  Seconds') \n",
    "            print_in_color (msg1 + msg2)\n",
    "            self.header=False\n",
    "        ep_end = time.time()\n",
    "        duration = ep_end - self.ep_start        \n",
    "        vloss=logs.get('val_loss')  # get the validation loss for this epoch\n",
    "        aloss=logs.get('loss')\n",
    "        acc = logs.get('accuracy')  # get training accuracy\n",
    "        v_acc = logs.get('val_accuracy')  # get validation accuracy\n",
    "        lr = float(tf.keras.backend.get_value(self.model.optimizer.lr)) # get the current learning rate\n",
    "        if epoch >0:\n",
    "            deltav = self.lowest_vloss- vloss \n",
    "            pimprov=(deltav/self.lowest_vloss) * 100\n",
    "            deltaa=self.lowest_aloss-aloss\n",
    "            aimprov=(deltaa/self.lowest_aloss) * 100            \n",
    "        else:\n",
    "            pimprov=0.0             \n",
    "        if vloss< self.lowest_vloss:\n",
    "            self.lowest_vloss=vloss\n",
    "            self.best_weights=self.model.get_weights() # set best weights to model's initial weights\n",
    "            self.best_epoch=epoch + 1 \n",
    "            new_lr=lr\n",
    "            msg = '{0:^7s}{1:^9.4f}{2:^9.2f}{3:^9.4f}{4:^10.2f}{5:^13.2f}{6:^10.6f}{7:11.6f}{8:^15.2f}'\n",
    "            msg=msg.format(str(epoch+1), aloss, acc*100, vloss, v_acc*100, pimprov, lr, new_lr,duration)         \n",
    "            print_in_color(msg, (0,255,0)) # green foreground\n",
    "        else: # validation loss increased                     \n",
    "            if self.dwell: # if dwell is True when the validation loss increases the learning rate is automatically reduced and model weights are set to best weights\n",
    "                lr=float(tf.keras.backend.get_value(self.model.optimizer.lr)) # get the current learning rate\n",
    "                new_lr=lr * self.factor\n",
    "                msg = '{0:^7s}{1:^9.4f}{2:^9.2f}{3:^9.4f}{4:^10.2f}{5:^13.2f}{6:^10.6f}{7:11.6f}{8:^15.2f}'                 \n",
    "                msg=msg.format(str(epoch+1), aloss, acc*100, vloss, v_acc*100, pimprov, lr, new_lr,duration) \n",
    "                print_in_color(msg, (255,255,0))                \n",
    "                tf.keras.backend.set_value(self.model.optimizer.lr, new_lr) # set the learning rate in the optimizer\n",
    "                self.model.set_weights(self.best_weights) # set the weights of the model to the best weights                 \n",
    "        if self.ask: # are the conditions right to query the user?\n",
    "            if epoch + 1 ==self.ask_epoch: # is this epoch the one for quering the user?                \n",
    "                msg='\\n Enter H to end training or  an integer for the number of additional epochs to run then ask again'\n",
    "                print_in_color(msg) # cyan foreground\n",
    "                ans=input()\n",
    "                if ans == 'H' or ans =='h' or ans == '0': # quit training for these conditions\n",
    "                    msg=f'you entered {ans},  Training halted on epoch {epoch+1} due to user input\\n'\n",
    "                    print_in_color(msg)\n",
    "                    self.model.stop_training = True # halt training\n",
    "                else: # user wants to continue training\n",
    "                    self.header=True\n",
    "                    self.ask_epoch += int(ans)\n",
    "                    msg=f'you entered {ans} Training will continue to epoch {self.ask_epoch}'\n",
    "                    print_in_color(msg) # cyan foreground\n",
    "                    if self.dwell==False:\n",
    "                        lr=float(tf.keras.backend.get_value(self.model.optimizer.lr)) # get the current learning rate\n",
    "                        msg=f'current LR is  {lr:8.6f}  hit enter to keep  this LR or enter a new LR'\n",
    "                        print_in_color(msg) # cyan foreground\n",
    "                        ans=input(' ')\n",
    "                        if ans =='':\n",
    "                            msg=f'keeping current LR of {lr:7.5f}'\n",
    "                            print_in_color(msg) # cyan foreground\n",
    "                        else:\n",
    "                            new_lr=float(ans)\n",
    "                            tf.keras.backend.set_value(self.model.optimizer.lr, new_lr) # set the learning rate in the optimizer\n",
    "                            msg=f' changing LR to {ans}'\n",
    "                            print_in_color(msg) # cyan foreground"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OK lets instantiate the callback and train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-12T11:41:46.694745Z",
     "iopub.status.busy": "2023-05-12T11:41:46.694031Z",
     "iopub.status.idle": "2023-05-12T13:09:47.715858Z",
     "shell.execute_reply": "2023-05-12T13:09:47.714763Z",
     "shell.execute_reply.started": "2023-05-12T11:41:46.694711Z"
    }
   },
   "outputs": [],
   "source": [
    "ask_epoch= 10 # initially train for 10 epochs   \n",
    "batches=int(len(train_df)/bs)\n",
    "# instantiate the custom callback\n",
    "epochs=100 # max epochs to run\n",
    "ask=LR_ASK(model, epochs=epochs,  ask_epoch=ask_epoch, batches=batches) # instantiate the custom callback\n",
    "callbacks=[ask] \n",
    "# train the model- don't worry aboutthe warning message your model will train correctly\n",
    "history=model.fit(x=train_gen,   epochs=epochs, verbose=0, callbacks=callbacks,  validation_data=valid_gen,\n",
    "               validation_steps=None,  shuffle=True,  initial_epoch=0) # train the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### lets define a function to plot the models training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-12T13:10:01.668727Z",
     "iopub.status.busy": "2023-05-12T13:10:01.668327Z",
     "iopub.status.idle": "2023-05-12T13:10:24.088042Z",
     "shell.execute_reply": "2023-05-12T13:10:24.087008Z",
     "shell.execute_reply.started": "2023-05-12T13:10:01.668697Z"
    }
   },
   "outputs": [],
   "source": [
    "def tr_plot(tr_data):\n",
    "    start_epoch=0\n",
    "    #Plot the training and validation data\n",
    "    tacc=tr_data.history['accuracy']\n",
    "    tloss=tr_data.history['loss']\n",
    "    vacc=tr_data.history['val_accuracy']\n",
    "    vloss=tr_data.history['val_loss']\n",
    "    tf1=tr_data.history['F1_score']\n",
    "    vf1=tr_data.history['val_F1_score']    \n",
    "    Epoch_count=len(tacc)+ start_epoch\n",
    "    Epochs=[]\n",
    "    for i in range (start_epoch ,Epoch_count):\n",
    "        Epochs.append(i+1)   \n",
    "    index_loss=np.argmin(vloss)#  this is the epoch with the lowest validation loss\n",
    "    val_lowest=vloss[index_loss]\n",
    "    index_acc=np.argmax(vacc)\n",
    "    acc_highest=vacc[index_acc]\n",
    "    indexf1=np.argmax(vf1)\n",
    "    vf1_highest=vf1[indexf1]\n",
    "    plt.style.use('fivethirtyeight')\n",
    "    sc_label='best epoch= '+ str(index_loss+1 +start_epoch)\n",
    "    vc_label='best epoch= '+ str(index_acc + 1+ start_epoch) \n",
    "    f1_label='best epoch= '+ str(index_acc + 1+ start_epoch)\n",
    "    fig,axes=plt.subplots(nrows=1, ncols=3, figsize=(25,10))\n",
    "    axes[0].plot(Epochs,tloss, 'r', label='Training loss')\n",
    "    axes[0].plot(Epochs,vloss,'g',label='Validation loss' )\n",
    "    axes[0].scatter(index_loss+1 +start_epoch,val_lowest, s=150, c= 'blue', label=sc_label)\n",
    "    axes[0].scatter(Epochs, tloss, s=100, c='red')    \n",
    "    axes[0].set_title('Training and Validation Loss')\n",
    "    axes[0].set_xlabel('Epochs', fontsize=18)\n",
    "    axes[0].set_ylabel('Loss', fontsize=18)\n",
    "    axes[0].legend()\n",
    "    axes[1].plot (Epochs,tacc,'r',label= 'Training Accuracy')\n",
    "    axes[1].scatter(Epochs, tacc, s=100, c='red')\n",
    "    axes[1].plot (Epochs,vacc,'g',label= 'Validation Accuracy')\n",
    "    axes[1].scatter(index_acc+1 +start_epoch,acc_highest, s=150, c= 'blue', label=vc_label)\n",
    "    axes[1].set_title('Training and Validation Accuracy')\n",
    "    axes[1].set_xlabel('Epochs', fontsize=18)\n",
    "    axes[1].set_ylabel('Accuracy', fontsize=18)\n",
    "    axes[1].legend()\n",
    "    axes[2].plot (Epochs,tf1,'r',label= 'Training F1 score')    \n",
    "    axes[2].plot (Epochs,vf1,'g',label= 'Validation F1 score')\n",
    "    index_tf1=np.argmax(tf1)#  this is the epoch with the highest training F1 score\n",
    "    tf1max=tf1[index_tf1]\n",
    "    index_vf1=np.argmax(vf1)# thisiis the epoch with the highest validation F1 score\n",
    "    vf1max=vf1[index_vf1]\n",
    "    axes[2].scatter(index_vf1+1 +start_epoch,vf1max, s=150, c= 'blue', label=vc_label)    \n",
    "    axes[2].scatter(Epochs, tf1, s=100, c='red')\n",
    "    axes[2].set_title('Training and Validation F1 score')\n",
    "    axes[2].set_xlabel('Epochs', fontsize=18)\n",
    "    axes[2].set_ylabel('F1  score', fontsize=18)\n",
    "    axes[2].legend()    \n",
    "    plt.tight_layout    \n",
    "    plt.show()\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-12T13:10:25.090323Z",
     "iopub.status.busy": "2023-05-12T13:10:25.089862Z",
     "iopub.status.idle": "2023-05-12T13:10:25.997809Z",
     "shell.execute_reply": "2023-05-12T13:10:25.996976Z",
     "shell.execute_reply.started": "2023-05-12T13:10:25.090283Z"
    }
   },
   "outputs": [],
   "source": [
    "tr_plot(history) # plot the training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now lets define a function to use the trained model to make predictions on the test set and produce a classification report and a confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-12T13:15:22.264599Z",
     "iopub.status.busy": "2023-05-12T13:15:22.264221Z",
     "iopub.status.idle": "2023-05-12T13:15:22.283395Z",
     "shell.execute_reply": "2023-05-12T13:15:22.281885Z",
     "shell.execute_reply.started": "2023-05-12T13:15:22.26457Z"
    }
   },
   "outputs": [],
   "source": [
    "def predictor(model,test_gen):\n",
    "    classes=list(test_gen.class_indices.keys())\n",
    "    class_count=len(classes)\n",
    "    preds=model.predict(test_gen, verbose=1)\n",
    "    errors=0\n",
    "    test_count =len(preds)\n",
    "    misclassified_classes=[]\n",
    "    misclassified_files=[]\n",
    "    misclassified_as = []\n",
    "    pred_indices=[]\n",
    "    for i, p in enumerate (preds):\n",
    "        pred_index=np.argmax(p)\n",
    "        pred_indices.append(pred_index)\n",
    "        true_index= test_gen.labels[i]    \n",
    "        if  pred_index != true_index:        \n",
    "            errors +=1        \n",
    "            misclassified_classes.append(classes[true_index])\n",
    "            misclassified_as.append(classes[pred_index])\n",
    "            file=test_gen.filenames[i]\n",
    "            split=file.split('/')\n",
    "            L=len(split)           \n",
    "            f=split[L-2] +' '+ split[L-1]  \n",
    "            misclassified_files.append(f)\n",
    "\n",
    "    accuracy = (test_count-errors)*100/test_count\n",
    "    ytrue=np.array(test_gen.labels)\n",
    "    ypred=np.array(pred_indices)\n",
    "    f1score=f1_score(ytrue, ypred, average='weighted')* 100\n",
    "    msg=f'There were {errors} errors in {test_count} tests for an accuracy of {accuracy:6.2f} and an F1 score of {f1score:6.2f}'\n",
    "    print (msg) \n",
    "    misclassified_classes=sorted(misclassified_classes)\n",
    "    if len(misclassified_classes) > 0:\n",
    "        misclassifications=[]\n",
    "        for klass in misclassified_classes:\n",
    "            mis_count=misclassified_classes.count(klass)\n",
    "            misclassifications.append(mis_count)\n",
    "        unique=len(np.unique(misclassified_classes)) \n",
    "        if unique==1:\n",
    "            height=int(unique)\n",
    "        else:\n",
    "            height =int(unique/2)\n",
    "        plt.figure(figsize=(10, height))\n",
    "        plt.style.use('fivethirtyeight')\n",
    "        plt.barh(misclassified_classes, misclassifications )\n",
    "        plt.title( 'Classification Errors on Test Set by Class', fontsize=20, color='blue')\n",
    "        plt.xlabel('NUMBER OF MISCLASSIFICATIONS', fontsize=20, color='blue')\n",
    "        plt.ylabel('CLASS', fontsize=20, color='blue')\n",
    "        plt.show()\n",
    "    if class_count <=30:\n",
    "        cm = confusion_matrix(ytrue, ypred )\n",
    "        # plot the confusion matrix\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        sns.heatmap(cm, annot=True, vmin=0, fmt='g', cmap='Blues', cbar=False)       \n",
    "        plt.xticks(np.arange(class_count)+.5, classes, rotation=90)\n",
    "        plt.yticks(np.arange(class_count)+.5, classes, rotation=0)\n",
    "        plt.xlabel(\"Predicted\")\n",
    "        plt.ylabel(\"Actual\")\n",
    "        plt.title(\"Confusion Matrix\")\n",
    "        plt.show()\n",
    "        clr = classification_report(ytrue, ypred, target_names=classes, digits= 4) # create classification report\n",
    "        print(\"Classification Report:\\n----------------------\\n\", clr)\n",
    "    return f1score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-12T13:20:19.788273Z",
     "iopub.status.busy": "2023-05-12T13:20:19.787883Z",
     "iopub.status.idle": "2023-05-12T13:20:52.585853Z",
     "shell.execute_reply": "2023-05-12T13:20:52.584794Z",
     "shell.execute_reply.started": "2023-05-12T13:20:19.788243Z"
    }
   },
   "outputs": [],
   "source": [
    "f1score=predictor(model,test_gen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lets define a function to save the model with a nomenclature of\n",
    "### subject-num of classes-image size-f1 score.h5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-12T13:31:53.256271Z",
     "iopub.status.busy": "2023-05-12T13:31:53.255874Z",
     "iopub.status.idle": "2023-05-12T13:31:53.26473Z",
     "shell.execute_reply": "2023-05-12T13:31:53.263607Z",
     "shell.execute_reply.started": "2023-05-12T13:31:53.25624Z"
    }
   },
   "outputs": [],
   "source": [
    "def save_model(model,subject, classes, img_size, f1score, working_dir):    \n",
    "    name=f'{subject}-{str(len(classes))}-({str(img_size[0])} X {str(img_size[1])})- {f1score:5.2f}.keras'    \n",
    "    model_save_loc=os.path.join(working_dir, name)    \n",
    "    try:\n",
    "        model.save(model_save_loc)        \n",
    "        msg= f'model was saved as {model_save_loc}'\n",
    "        print_in_color(msg, (0,255,255),(0,0,0))\n",
    "    except:\n",
    "        msg='model can not be saved due to tensorflow 2.10.0 or higher. Bug involving use of EfficientNet Models'        \n",
    "        print_in_color(msg, (0,255,255), (0,0,0)) # cyan foreground"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-12T13:31:55.489375Z",
     "iopub.status.busy": "2023-05-12T13:31:55.48899Z",
     "iopub.status.idle": "2023-05-12T13:31:56.388291Z",
     "shell.execute_reply": "2023-05-12T13:31:56.387094Z",
     "shell.execute_reply.started": "2023-05-12T13:31:55.489343Z"
    }
   },
   "outputs": [],
   "source": [
    "subject='CIFAKE'\n",
    "working_dir=r'/ai-image-detection/models/'\n",
    "save_model(model,subject, classes, img_size, f1score, working_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
